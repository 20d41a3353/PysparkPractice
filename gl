

import sys
from datetime import date, timedelta
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import expr, col, when, substring, concat, lit
from pyspark.sql.types import DecimalType, StringType, StructType, StructField, LongType

# NUM_PARTITIONS = 1000          # partitions for heavy groupBy (tune to 2-4x total cores)
# WRITE_PARTITIONS = 50         # partitions used for JDBC write
# JDBC_BATCHSIZE = 50000        # JDBC batch size for writes
# SPARK_SHUFFLE_PARTITIONS = 1500


NUM_PARTITIONS = 24           # partitions for heavy groupBy (4x total cores)
WRITE_PARTITIONS = 6          # partitions used for JDBC write (1x total cores)
JDBC_BATCHSIZE = 45000        # JDBC batch size for writes
SPARK_SHUFFLE_PARTITIONS = 24 # Match NUM_PARTITIONS for efficiency

oracle_url = "jdbc:oracle:thin:@//10.177.103.192:1523/fincorepdb1"
oracle_user = "fincore"
oracle_password = "Password#1234"
oracle_driver = "oracle.jdbc.driver.OracleDriver"

hdfs_base_path = "hdfs://10.177.103.199:8022/CBS-FILES/2025-11-28/GLIF/"

spark = SparkSession.builder \
    .appName("HDFS_Text_to_Oracle_Optimized") \
    .config("spark.jars", "./ojdbc8.jar") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.shuffle.partitions", str(SPARK_SHUFFLE_PARTITIONS)) \
    .config("spark.sql.files.maxPartitionBytes", "256MB") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

today = date.today()
yesterday = today - timedelta(days=1)
posting_date_str = yesterday.strftime("%Y-%m-%d")


def fetch_next_batch_id():
    try:
        df_batch_id = spark.read.format("jdbc") \
            .option("url", oracle_url) \
            .option("query", "SELECT get_next_batch_id AS BATCH_ID_VAL FROM DUAL") \
            .option("user", oracle_user) \
            .option("password", oracle_password) \
            .option("driver", oracle_driver) \
            .option("fetchsize", 1) \
            .load()

        batch_id_value = df_batch_id.collect()[0]["BATCH_ID_VAL"]
        return int(batch_id_value)
    except Exception as e:
        raise
file_types_with_extra = ["INV", "PRI", "BOR", "PRB", "ONL", "AGRI_PY", "AGRI_CY", "INV_IN","INV_TD","INV_DC","BOR_DC"]
file_types_without_extra = ["INV_BC", "INV_FD", "INV_RE", "GEN_FIT", "INV_B8", "INV_CT", "INV_ECG", "INV_FCI","BOR_VS","INV_GB"]
paths_to_read = [f"{hdfs_base_path}*GLIF{ft}_[0-9][0-9][0-9].gz" for ft in file_types_with_extra]
paths_to_read.extend([f"{hdfs_base_path}*GLIF{ft}.gz" for ft in file_types_without_extra])

def parse_and_aggregate(batch_id_literal):
    df_raw = spark.read.text(paths_to_read)
    # df_raw = spark.read.text("hdfs://10.177.103.199:8022/CBS-FILES/2025-11-28/GLIF/VARAGA_GLIFONL_351*")

    df_parsed = df_raw.select(
        substring("value", 51, 18).alias("Id"),
        substring("value", 48, 3).alias("currency_code"),
        when(substring("value", 48, 3) != "INR",
             substring("value", 133, 17)).otherwise(substring("value", 116, 17)).alias("Amount_raw"),
        when(substring("value", 48, 3) != "INR",
             substring("value", 149, 1)).otherwise(substring("value", 132, 1)).alias("sign_char")
    ).select(
        "Id",
        "currency_code",
        substring("Amount_raw", 1, 16).alias("Amount_base"),
        substring("Amount_raw", 17, 1).alias("sign_char")
    )

    try:
        df_raw.unpersist()
    except Exception:
        pass

    df_amounts = df_parsed.withColumn(
        "last_digit", F.translate(col("sign_char"), "pqrstuvwxy", "0123456789")
    ).withColumn(
        "sign", when(col("sign_char").isin(list("pqrstuvwxy")), lit(-1)).otherwise(lit(1))
    ).withColumn(
        "Amount_decimal_str", concat(col("Amount_base"), col("last_digit"))
    ).withColumn(
        "Amount_final", (col("Amount_decimal_str").cast(DecimalType(22, 3)) / lit(1000)) * col("sign")
    ).select("Id", "currency_code", "Amount_final")

    try:
        df_parsed.unpersist()
    except Exception:
        pass

    df_signed = df_amounts.withColumn("DEBIT", when(col("Amount_final") > 0, col("Amount_final")).otherwise(lit(0))) \
                          .withColumn("CREDIT", when(col("Amount_final") < 0, col("Amount_final")).otherwise(lit(0))) \
                          .withColumn("ONE", lit(1))

    df_repart = df_signed.repartition(SPARK_SHUFFLE_PARTITIONS, "Id").persist()

    df_agg = df_repart.groupBy("Id").agg(
        F.sum("DEBIT").alias("DEBIT_AMOUNT"),
        F.sum("CREDIT").alias("CREDIT_AMOUNT"),
        F.sum("ONE").alias("TRANSACTION_COUNT")
    )

    try:
        df_repart.unpersist()
    except Exception:
        pass

    df_mapped = df_agg.select(
        lit(str(batch_id_literal)).alias("BATCH_ID").cast(StringType()),
        lit(None).cast(StringType()).alias("JOURNAL_ID"),
        substring("Id", 1, 5).alias("BRANCH_CODE"),
        substring("Id", 6, 3).alias("CURRENCY"),
        substring("Id", 9, 10).alias("CGL"),
        lit("CBS consolidated txns").alias("NARRATION"),
        "DEBIT_AMOUNT",
        "CREDIT_AMOUNT",
        "TRANSACTION_COUNT",
        lit("C").alias("SOURCE_FLAG")
    )

    return df_mapped

def read_lookup_tables():
    branch_query = "(SELECT code FROM branch_master) T1"
    branch_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", branch_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize", 10000) \
        .load()

    currency_query = "(SELECT CURRENCY_CODE FROM currency_master) T1"
    currency_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", currency_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize", 10000) \
        .load()

    cgl_query = "(SELECT CGL_NUMBER FROM cgl_master WHERE BAL_COMPARE = 1) T1"
    master_cgl_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", cgl_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize", 10000) \
        .load()

    return branch_list, currency_list, master_cgl_list

def validate_and_collapse(df_mapped, branch_list, currency_list, master_cgl_list):
    branch_bcast = F.broadcast(branch_list.select(col("CODE").alias("BRANCH_CODE")).withColumn("IS_VALID_BRANCH", lit(1)))
    currency_bcast = F.broadcast(currency_list.select(col("CURRENCY_CODE").alias("CURRENCY")).withColumn("IS_VALID_CURRENCY", lit(1)))
    cgl_bcast = F.broadcast(master_cgl_list.select(col("CGL_NUMBER").alias("CGL")).withColumn("IS_VALID", lit(1)))

    df_valid_branch = df_mapped.join(branch_bcast, on="BRANCH_CODE", how="left") \
        .withColumn("VALIDATED_BRANCH", when(col("IS_VALID_BRANCH").isNotNull(), col("BRANCH_CODE")).otherwise(lit("invalid_BRANCH")))

    df_valid_currency = df_valid_branch.join(currency_bcast, on="CURRENCY", how="left") \
        .withColumn("VALIDATED_CURRENCY", when(col("IS_VALID_CURRENCY").isNotNull(), col("CURRENCY")).otherwise(lit("invalid_CURRENCY")))

    df_validated = df_valid_currency.join(cgl_bcast, on="CGL", how="left") \
        .withColumn(
            "VALIDATED_CGL",
            when(col("IS_VALID").isNotNull(), col("CGL"))
            .when(col("CGL").startswith("5"), lit("5000000000"))
            .otherwise(lit("1111111111"))
        ).withColumn(
            "NARRATION1",
            when(col("IS_VALID").isNull(), concat(lit("INVALID CGL: "), col("CGL"), lit(" - "), col("NARRATION"))).otherwise(col("NARRATION"))
        )

    df_validated = df_validated.drop("IS_VALID_BRANCH", "IS_VALID_CURRENCY", "IS_VALID")

    result = df_validated.groupBy("VALIDATED_CGL", "CURRENCY", "BRANCH_CODE").agg(
        F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
        F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
        F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT"),
        F.first("NARRATION1").alias("NARRATION"),
        F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),
        F.first("BATCH_ID").alias("BATCH_ID"),
        F.first("JOURNAL_ID").alias("JOURNAL_ID")
    ).select(
        F.col("VALIDATED_CGL").alias("CGL"),
        "CURRENCY", "BRANCH_CODE", "NARRATION", "SOURCE_FLAG", "BATCH_ID",
        "DEBIT_AMOUNT", "CREDIT_AMOUNT", "TRANSACTION_COUNT", "JOURNAL_ID"
    )

    result_net1 = result.withColumn("check", when(substring(col("CGL"), 1, 1) == "5", lit("5000000000")).otherwise(lit("1111111111")))

    new_result = result_net1.groupBy("BRANCH_CODE", "CURRENCY", col("check").alias("CGL")).agg(
        F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
        F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
        F.first("NARRATION").alias("NARRATION"),
        F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),
        F.first("BATCH_ID").alias("BATCH_ID"),
        F.first("JOURNAL_ID").alias("JOURNAL_ID"),
        F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT")
    ).withColumn("NET", col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT"))

    synthetic = new_result.filter(col("NET") != 0).select(
        col("CGL"), col("CURRENCY"), col("BRANCH_CODE"),
        when(col("NET") < 0, -col("NET")).otherwise(lit(0)).alias("DEBIT_AMOUNT"),
        when(col("NET") > 0, -col("NET")).otherwise(lit(0)).alias("CREDIT_AMOUNT"),
        col("TRANSACTION_COUNT"),
        lit("OUT OF BALANCE").alias("NARRATION"),
        col("SOURCE_FLAG"),
        col("BATCH_ID"),
        col("JOURNAL_ID")
    )
    # result.show(20, False)
    # synthetic.show(20, False)


    final_balanced = result.unionByName(synthetic).withColumn("TRANSACTION_DATE", F.to_date(F.lit(posting_date_str)))

    return final_balanced

def write_to_jdbc(df, table_name, partitions=WRITE_PARTITIONS, mode="append"):
    df_for_write = df.repartition(partitions, "BRANCH_CODE")
    try:
        df_for_write.write \
            .format("jdbc") \
            .option("url", oracle_url) \
            .option("dbtable", table_name) \
            .option("user", oracle_user) \
            .option("password", oracle_password) \
            .option("driver", oracle_driver) \
            .option("batchsize", JDBC_BATCHSIZE) \
            .mode(mode) \
            .save()
    except Exception as e:
        raise

def update_gl_balance(final_balanced):
    processed_df = final_balanced.withColumn("BALANCE", col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")) \
                                 .select("CGL", "BALANCE", "CURRENCY", "BRANCH_CODE")
    filter_date_str = posting_date_str
    sql_query = f"(SELECT CGL, BALANCE, CURRENCY, BRANCH_CODE FROM GL_BALANCE WHERE TRUNC(BALANCE_DATE) = TO_DATE('{filter_date_str}','YYYY-MM-DD')) T1"

    df_date_filtered = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", sql_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize", 10000) \
        .load()

    combined_df = processed_df.unionByName(df_date_filtered)
    final_aggregated_df = combined_df.groupBy("CGL", "CURRENCY", "BRANCH_CODE").agg(F.sum("BALANCE").alias("BALANCE")) \
        .orderBy("CGL", "BRANCH_CODE").withColumn("BALANCE_DATE", F.to_date(F.lit(posting_date_str)))
    # GL_BALANCE
    write_to_jdbc(final_aggregated_df, "GL_BAL")

def main():
    batch_id_literal = fetch_next_batch_id()

    df_mapped = parse_and_aggregate(batch_id_literal)

    branch_list, currency_list, master_cgl_list = read_lookup_tables()

    final_balanced = validate_and_collapse(df_mapped, branch_list, currency_list, master_cgl_list)

    # GL_TRANSACTIONS
    write_to_jdbc(final_balanced, "GL_TRANS", partitions=WRITE_PARTITIONS)

    update_gl_balance(final_balanced)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        raise
    finally:
        try:
            spark.catalog.clearCache()
        except Exception:
            pass
        spark.stop()
