import datetime 
from pyspark.sql import SparkSession , functions as F
from pyspark.sql.functions import expr, col,when, length, substring,trim, concat, lit, create_map, to_timestamp, sum,broadcast
from pyspark.sql.types import DecimalType, StringType, StructType, StructField, LongType 
from datetime import date, timedelta

# Get today's date
today = date.today()

# Calculate yesterday's date
# timedelta(days=1) represents a difference of one day
yesterday = today - timedelta(days=1)

spark = SparkSession.builder \
    .appName("HDFS_Text_to_Oracle") \
    .config("spark.jars", "./ojdbc8.jar") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://10.177.103.199:8022") \
    .getOrCreate()
    
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "400")
spark.conf.set("spark.sql.files.maxPartitionBytes", "256MB")
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

# oracle_url = "jdbc:oracle:thin:@//10.191.216.58:1523/crsprod"
# oracle_user = "ftwoahm"
# oracle_password = "Password@123"
# oracle_driver = "oracle.jdbc.driver.OracleDriver"

oracle_url = "jdbc:oracle:thin:@//10.177.103.192:1523/fincorepdb1"
oracle_user = "fincore"
oracle_password = "Password#1234"
oracle_driver = "oracle.jdbc.driver.OracleDriver"

DriverManager = spark._jvm.java.sql.DriverManager


batch_id_schema = StructType([
    StructField("BATCH_ID_VAL", LongType(), False) 
])

BATCH_ID_LITERAL = None
try:
    df_batch_id = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("query", "SELECT get_next_batch_id AS BATCH_ID_VAL FROM DUAL") \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize",1).load()

    batch_id_value = df_batch_id.collect()[0]["BATCH_ID_VAL"]
    BATCH_ID_LITERAL = int(batch_id_value)
    print(f"=== Fetched BATCH_ID: {BATCH_ID_LITERAL} ===")
except Exception as e:
    print(f"Error fetching BATCH_SEQ via JDBC: {e}")
    spark.stop()
    exit()

posting_date_str = yesterday.strftime("%Y-%m-%d")
# posting_date_str = datetime.date.today().strftime("%Y-%m-%d")
print(f"=== Using current date for POST_DATE: {posting_date_str} ===")


base_path = "hdfs://10.177.103.199:8022/CBS-FILES/2025-11-28/GLIF/"

file_types_with_extra = ["INV", "PRI", "BOR", "PRB", "ONL", "AGRI_PY", "AGRI_CY", "INV_IN","INV_TD","INV_DC","BOR_DC"]
file_types_without_extra = ["INV_BC", "INV_FD", "INV_RE", "GEN_FIT", "INV_B8", "INV_CT", "INV_ECG", "INV_FCI","BOR_VS","INV_GB"]
paths_to_read = [f"{base_path}*GLIF{ft}_[0-9][0-9][0-9].gz" for ft in file_types_with_extra]
paths_to_read.extend([f"{base_path}*GLIF{ft}.gz" for ft in file_types_without_extra])

df_raw = spark.read.text(paths_to_read)


# df_raw = spark.read.text("hdfs://10.177.103.199:8022/CBS-FILES/2025-11-28/GLIF/VARAGA_GLIFONL_351*")

digit_map = create_map(
    lit('1'), lit('1'), lit('2'), lit('2'), lit('3'), lit('3'), lit('4'), lit('4'), lit('5'), lit('5'),
    lit('6'), lit('6'), lit('7'), lit('7'), lit('8'), lit('8'), lit('9'), lit('9'), lit('0'), lit('0'),
    lit('p'), lit('0'), lit('q'), lit('1'), lit('r'), lit('2'), lit('s'), lit('3'), lit('t'), lit('4'),
    lit('u'), lit('5'), lit('v'), lit('6'), lit('w'), lit('7'), lit('x'), lit('8'), lit('y'), lit('9')
)
df_processed = df_raw.withColumns({
    "Id": expr("substring(value, 51, 18)"),
    "currency_code": expr("substring(value, 48, 3)")
})
df_clean = df_processed.withColumns({
    "Amount_raw": when(col("currency_code") != "INR", expr("substring(value, 133, 17)"))
                 .otherwise(expr("substring(value, 116, 17)")),
    "last_char": when(col("currency_code") != "INR", expr("substring(value, 149, 1)"))
                 .otherwise(expr("substring(value, 132, 1)"))
}).drop("value", "currency_code")
df_clean = df_clean.withColumns({
    "Amount_base": substring(col("Amount_raw"), 1, 16),
    "sign_char": substring(col("Amount_raw"), 17, 1)
})


df_with_signed_amount = df_clean.withColumns({
    "last_digit": digit_map.getItem(col("sign_char")),
    "sign": when(col("sign_char").isin(['p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']), -1).otherwise(1)
})
df_with_signed_amount = df_with_signed_amount.withColumn(
    "Amount_decimal_str",
    concat(col("Amount_base"), col("last_digit"))
)
df_final = df_with_signed_amount.withColumns({
    "Amount_final": (col("Amount_decimal_str").cast(DecimalType(22, 3)) / 1000) * col("sign")
})
df_with_signed_amount = df_final.withColumns({
    "Amountpve": when(df_final["Amount_final"] > 0, df_final["Amount_final"]).otherwise(0),
    "Amountnve": when(df_final["Amount_final"] < 0, df_final["Amount_final"]).otherwise(0)
})
df_agg = df_with_signed_amount.groupBy("Id").agg(
    F.sum(F.col("Amountpve")).alias("DEBIT_AMOUNT"),
    F.sum(F.col("Amountnve")).alias("CREDIT_AMOUNT"),
    F.count(F.col("Id")).alias("TRANSACTION_COUNT")
)

df_mapped = df_agg.withColumns({ 
    "BATCH_ID": lit(BATCH_ID_LITERAL).cast(StringType()), 
    # "TRANSACTION_DATE": F.to_date(F.lit(posting_date_str)),
    "JOURNAL_ID": lit(None).cast(StringType()),
    "BRANCH_CODE": substring(col("Id"), 1, 5),
    "CURRENCY": substring(col("Id"), 6, 3),
    "CGL": substring(col("Id"), 9, 10),
    "NARRATION": lit("CBS consolidated txns").cast(StringType()),
    "SOURCE_FLAG": lit("C").cast(StringType())
})

df_final_schema = df_mapped.select(
    "BATCH_ID",
    "JOURNAL_ID",
    "BRANCH_CODE",
    "CURRENCY",
    "CGL",
    "NARRATION",
    "DEBIT_AMOUNT",
    "CREDIT_AMOUNT",
    "TRANSACTION_COUNT",
    "SOURCE_FLAG"
)



# ===================================================================================
                #    || BRANCH CODE ||
# ===================================================================================

branch_query = f"""
(
    SELECT code FROM branch_master 
) T1
"""
branch_list = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", branch_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 110000) \
    .option("numPartitions", 50) \
    .load()
    
df_validated_branch = df_final_schema.join(
    broadcast(branch_list.select(col("CODE").alias("BRANCH_CODE")).withColumn("IS_VALID_BRANCH", lit(1))),
    on="BRANCH_CODE",
    how="left"
).withColumn(
    "VALIDATED_BRANCH",
    when(col("IS_VALID_BRANCH").isNotNull(), col("BRANCH_CODE"))
    .otherwise(lit("invalid_BRANCH"))
)



df_invalid_branches = df_validated_branch.where(col("VALIDATED_BRANCH") == "invalid_BRANCH")



df_valid_branches = df_validated_branch.where(col("VALIDATED_BRANCH") != "invalid_BRANCH")


df_valid_branches =  df_valid_branches.drop(col("VALIDATED_BRANCH"),col("IS_VALID_BRANCH"))





# ===================================================================================
                #    || CURRENCY ||
# ===================================================================================


currency_query = f"""
(
    SELECT CURRENCY_CODE from currency_master
) T1
"""
currency_list = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", currency_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 110000) \
    .option("numPartitions", 50) \
    .load()

df_validated_currency = df_valid_branches.join(
    broadcast(currency_list.select(col("CURRENCY_CODE").alias("CURRENCY")).withColumn("IS_VALID_CURRENCY", lit(1))),
    on="CURRENCY",
    how="left"
).withColumn(
    "VALIDATED_CURRENCY",
    when(col("IS_VALID_CURRENCY").isNotNull(), col("CURRENCY"))
    .otherwise(lit("invalid_CURRENCY"))
)

df_invalid_currency = df_validated_currency.where(col("VALIDATED_CURRENCY") == "invalid_CURRENCY")


df_valid_currency = df_validated_currency.where(col("VALIDATED_CURRENCY") != "invalid_CURRENCY")


df_valid_currency =  df_valid_currency.drop(col("VALIDATED_CURRENCY"),col("IS_VALID_CURRENCY"))




# # ===================================================================================
#                 #    || CGL ||
# # ===================================================================================


cgl_query = f"""
(
    SELECT CGL_NUMBER FROM cgl_master where BAL_COMPARE=1
) T1
"""

master_cgl_list = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", cgl_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 110000) \
    .option("numPartitions", 50) \
    .load()
    

df_validated = df_valid_currency.join(
    broadcast(master_cgl_list.select(col("CGL_NUMBER").alias("CGL")).withColumn("IS_VALID", lit(1))),
    on="CGL",
    how="left"
).withColumn(
    "VALIDATED_CGL",
    when(col("IS_VALID").isNotNull(), col("CGL"))
    .when(col("CGL").startswith("5"), lit("5000000000"))
    .otherwise(lit("1111111111"))
).withColumn(
    "NARRATION1",   
    when(col("IS_VALID").isNull(), 
         concat(lit("INVALID CGL: "), col("CGL"), lit(" - "), col("NARRATION")))
    .otherwise(col("NARRATION"))
)
print("# ------------------ $"*20)
df_validated.show()
print("---------------------------------------$"*20)

import pyspark.sql.functions as F

result = df_validated.groupBy("VALIDATED_CGL", "CURRENCY", "BRANCH_CODE").agg(
    F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
    F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
    F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT"),
    F.first("NARRATION1").alias("NARRATION"),          # Add these lines
    F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),      # to aggregate
    F.first("BATCH_ID").alias("BATCH_ID"),
    F.first("JOURNAL_ID").alias("JOURNAL_ID"),
    # the columns
).select(F.col("VALIDATED_CGL").alias("CGL"), "CURRENCY", "BRANCH_CODE","NARRATION", "SOURCE_FLAG", "BATCH_ID", "DEBIT_AMOUNT", "CREDIT_AMOUNT", "TRANSACTION_COUNT","JOURNAL_ID")


# ===============================================================================================
# ===============================================================================================
# ===============================================================================================


result_net1 = result.withColumn(
    "check",
    when(substring(col("CGL"), 1, 1) == "5", lit("5000000000"))
    .otherwise(lit("1111111111"))
)


new_result =result_net1.groupBy("BRANCH_CODE","CURRENCY",col("check").alias("CGL")).agg(
    F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
    F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
    F.first("NARRATION").alias("NARRATION"),          
    F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),      
    F.first("BATCH_ID").alias("BATCH_ID"),
    F.first("JOURNAL_ID").alias("JOURNAL_ID"),
    F.first("TRANSACTION_COUNT").alias("TRANSACTION_COUNT")
    ).withColumn(
    "NET", 
    col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")
)



synthetic = new_result.filter(col("NET") != 0).select(
    col("CGL"),col("CURRENCY"),col("BRANCH_CODE"),
    when(col("NET") < 0, -col("NET")).otherwise(lit(0)).alias("DEBIT_AMOUNT"),
    when(col("NET") > 0, -col("NET")).otherwise(lit(0)).alias("CREDIT_AMOUNT"),
    col("TRANSACTION_COUNT"),
    F.lit("OUT OF BALANCE").alias("NARRATION"),
    col("SOURCE_FLAG"),
    col("BATCH_ID"),
    col("JOURNAL_ID"))

synthetic.show()
result.show()
print("# synthetic $"*20)
final_balanced = result.unionByName(synthetic).withColumn("TRANSACTION_DATE",F.to_date(F.lit(posting_date_str)))


final_balanced.show()

processed_df = final_balanced.withColumn(
    "BALANCE", 
    col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")
).select("CGL", "BALANCE","CURRENCY","BRANCH_CODE")

# ===============================================================================
# Adesh data from the database
# ===============================================================================

try:
    final_balanced.write \
            .format("jdbc") \
            .option("url", oracle_url) \
            .option("dbtable", "GL_TRANS") \
            .option("user", oracle_user) \
            .option("password", oracle_password) \
            .option("driver", oracle_driver) \
            .option("batchsize", 110000) \
            .option("numPartitions", 50) \
            .mode("append") \
            .save()





    print("=== Data successfully written to Oracle DB ===") 
except Exception as e:
    print(f"Error writing to Oracle DB: {e}")

# ====================================================================================================================================================================================================================================================================

# ====================================================================================================================================================================================================================================================================


# filter_date_str = '2025-11-06'
filter_date_str = yesterday

sql_query = f"""
(
    SELECT CGL,BALANCE,CURRENCY,BRANCH_CODE
    FROM GL_BALANCE 
    WHERE TRUNC(BALANCE_DATE) = TO_DATE('{filter_date_str}', 'YYYY-MM-DD')
) T1
"""




df_date_filtered = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", sql_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 110000) \
    .option("numPartitions", 50) \
    .load()



combined_df = processed_df.unionAll(df_date_filtered)

final_aggregated_df = combined_df.groupBy("CGL", "CURRENCY", "BRANCH_CODE").agg(sum("BALANCE").alias("BALANCE")).orderBy("CGL", "BRANCH_CODE")
final_aggregated_df=final_aggregated_df.withColumn("BALANCE_DATE",F.to_date(F.lit(posting_date_str)))

final_aggregated_df.write \
    .format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", "GL_BAL") \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 110000) \
    .option("numPartitions", 50) \
    .mode("append") \
    .save()

spark.stop()








