# ---------- Optimized parsing + aggregation snippet ----------
from pyspark.sql import functions as F
from pyspark.sql.functions import expr, col, when, substring, concat, lit, translate
from pyspark.sql.types import DecimalType, StringType

# ---- 1) Parse input in a single select (reduce intermediate DF objects) ----
# Note: substring(value, start, len) in Spark SQL uses 1-based indexing
df_raw = spark.read.text(paths_to_read)  # keep as you had

# parse relevant fields in one select
df_parsed = df_raw.select(
    substring("value", 51, 18).alias("Id"),
    substring("value", 48, 3).alias("currency_code"),
    # Amount_raw depends on currency: choose offset via when inside select
    when(substring("value", 48, 3) != "INR",
         substring("value", 133, 17)).otherwise(substring("value", 116, 17)).alias("Amount_raw"),
    # last char (sign)
    when(substring("value", 48, 3) != "INR",
         substring("value", 149, 1)).otherwise(substring("value", 132, 1)).alias("sign_char")
).select(
    "Id",
    "currency_code",
    substring("Amount_raw", 1, 16).alias("Amount_base"),
    substring("Amount_raw", 17, 1).alias("sign_char")
)

# free df_raw
df_raw.unpersist() if hasattr(df_raw, "unpersist") else None

# ---- 2) convert sign_char to digit and sign in a compact way ----
# mapping p->0 q->1 r->2 s->3 t->4 u->5 v->6 w->7 x->8 y->9
# translate works char->char: we need a 1-to-1 mapping
# We'll translate pqrstuvwxy -> 0123456789
df_amounts = df_parsed.withColumn(
    "last_digit", translate(col("sign_char"), lit("pqrstuvwxy"), lit("0123456789"))
).withColumn(
    "sign",
    when(col("sign_char").isin(['p','q','r','s','t','u','v','w','x','y']), F.lit(-1)).otherwise(F.lit(1))
).withColumn(
    # combine: Amount_base (16 chars) + last_digit -> string that becomes e.g. "000000000123450"
    "Amount_decimal_str", concat(col("Amount_base"), col("last_digit"))
).withColumn(
    "Amount_final",
    (col("Amount_decimal_str").cast(DecimalType(22, 3)) / F.lit(1000)) * col("sign")
).select(
    "Id",
    "currency_code",
    "Amount_final"
)

# Unpersist df_parsed if needed
df_parsed.unpersist() if hasattr(df_parsed, "unpersist") else None

# ---- 3) Split positive and negative once (avoid extra wide DF) ----
df_signed = df_amounts.withColumn("DEBIT", when(col("Amount_final") > 0, col("Amount_final")).otherwise(F.lit(0))) \
                      .withColumn("CREDIT", when(col("Amount_final") < 0, col("Amount_final")).otherwise(F.lit(0))) \
                      .withColumn("ONE", F.lit(1))

# ---- 4) Repartition by Id (or a hashed key) BEFORE groupBy to avoid skew ----
# Choose partitions according to cluster capacity, e.g., 400
NUM_PARTITIONS = 400
df_repart = df_signed.repartition(NUM_PARTITIONS, "Id")

# Persist if reused; here we aggregate next so persist helps avoid re-computation
df_repart = df_repart.persist()

# ---- 5) Aggregation by Id (map-side combine + shuffle) ----
df_agg = df_repart.groupBy("Id").agg(
    F.sum("DEBIT").alias("DEBIT_AMOUNT"),
    F.sum("CREDIT").alias("CREDIT_AMOUNT"),
    F.sum("ONE").alias("TRANSACTION_COUNT")
)

# release memory for df_repart
df_repart.unpersist()

# ---- 6) map Id -> components and add static columns (BATCH_ID literal is fine) ----
df_mapped = df_agg.select(
    lit(str(BATCH_ID_LITERAL)).alias("BATCH_ID").cast(StringType()),
    F.lit(None).cast(StringType()).alias("JOURNAL_ID"),
    substring("Id", 1, 5).alias("BRANCH_CODE"),
    substring("Id", 6, 3).alias("CURRENCY"),
    substring("Id", 9, 10).alias("CGL"),
    F.lit("CBS consolidated txns").alias("NARRATION"),
    "DEBIT_AMOUNT",
    "CREDIT_AMOUNT",
    "TRANSACTION_COUNT",
    F.lit("C").alias("SOURCE_FLAG")
)

# ---- 7) Join with branch/currency/cgl lookups (broadcast, since they are small) ----
# If branch_list/currency_list/master_cgl_list are small, broadcasting is fine
branch_bcast = broadcast(branch_list.select(col("CODE").alias("BRANCH_CODE")).withColumn("IS_VALID_BRANCH", lit(1)))
currency_bcast = broadcast(currency_list.select(col("CURRENCY_CODE").alias("CURRENCY")).withColumn("IS_VALID_CURRENCY", lit(1)))
cgl_bcast = broadcast(master_cgl_list.select(col("CGL_NUMBER").alias("CGL")).withColumn("IS_VALID", lit(1)))

df_valid_branch = df_mapped.join(branch_bcast, on="BRANCH_CODE", how="left") \
    .withColumn("VALIDATED_BRANCH", when(col("IS_VALID_BRANCH").isNotNull(), col("BRANCH_CODE")).otherwise(lit("invalid_BRANCH")))

df_valid_currency = df_valid_branch.join(currency_bcast, on="CURRENCY", how="left") \
    .withColumn("VALIDATED_CURRENCY", when(col("IS_VALID_CURRENCY").isNotNull(), col("CURRENCY")).otherwise(lit("invalid_CURRENCY")))

df_validated = df_valid_currency.join(cgl_bcast, on="CGL", how="left") \
    .withColumn(
        "VALIDATED_CGL",
        when(col("IS_VALID").isNotNull(), col("CGL"))
        .when(col("CGL").startswith("5"), lit("5000000000"))
        .otherwise(lit("1111111111"))
    ).withColumn(
        "NARRATION1",
        when(col("IS_VALID").isNull(), concat(lit("INVALID CGL: "), col("CGL"), lit(" - "), col("NARRATION"))).otherwise(col("NARRATION"))
    )

# Drop intermediate markers and persist small df for later grouping
df_validated = df_validated.drop("IS_VALID_BRANCH", "IS_VALID_CURRENCY", "IS_VALID")

# ---- 8) Group by validated CGL/CURRENCY/BRANCH to combine duplicates (reduce further shuffle) ----
result = df_validated.groupBy("VALIDATED_CGL", "CURRENCY", "BRANCH_CODE").agg(
    F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
    F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
    F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT"),
    F.first("NARRATION1").alias("NARRATION"),
    F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),
    F.first("BATCH_ID").alias("BATCH_ID"),
    F.first("JOURNAL_ID").alias("JOURNAL_ID")
).select(
    F.col("VALIDATED_CGL").alias("CGL"),
    "CURRENCY", "BRANCH_CODE", "NARRATION", "SOURCE_FLAG", "BATCH_ID",
    "DEBIT_AMOUNT", "CREDIT_AMOUNT", "TRANSACTION_COUNT", "JOURNAL_ID"
)

# ---- 9) Collapse into final buckets and create synthetic rows where NET != 0 ----
result_net1 = result.withColumn(
    "check", when(substring(col("CGL"), 1, 1) == "5", lit("5000000000")).otherwise(lit("1111111111"))
)

new_result = result_net1.groupBy("BRANCH_CODE", "CURRENCY", col("check").alias("CGL")).agg(
    F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
    F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
    F.first("NARRATION").alias("NARRATION"),
    F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),
    F.first("BATCH_ID").alias("BATCH_ID"),
    F.first("JOURNAL_ID").alias("JOURNAL_ID"),
    F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT")
).withColumn("NET", col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT"))

synthetic = new_result.filter(col("NET") != 0).select(
    col("CGL"), col("CURRENCY"), col("BRANCH_CODE"),
    when(col("NET") < 0, -col("NET")).otherwise(lit(0)).alias("DEBIT_AMOUNT"),
    when(col("NET") > 0, -col("NET")).otherwise(lit(0)).alias("CREDIT_AMOUNT"),
    col("TRANSACTION_COUNT"),
    F.lit("OUT OF BALANCE").alias("NARRATION"),
    col("SOURCE_FLAG"),
    col("BATCH_ID"),
    col("JOURNAL_ID")
)

final_balanced = new_result.unionByName(synthetic).withColumn("TRANSACTION_DATE", F.to_date(F.lit(posting_date_str)))

# ---- 10) Partition control before JDBC writes ----
# Repartition to a number of partitions appropriate for parallel JDBC writes.
# Use fewer partitions for many small writes; more partitions to parallelize large writes.
WRITE_PARTITIONS = 50
final_for_write = final_balanced.repartition(WRITE_PARTITIONS, "BRANCH_CODE")  # choose a column for locality

# Write GL_TRANS - control batchsize and partitions
final_for_write.write \
    .format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", "GL_TRANS") \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 10000) \
    .mode("append") \
    .save()

# ---- 11) GL balance update: compute processed_df small and union with db data ----
processed_df = final_balanced.withColumn("BALANCE", col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")) \
                             .select("CGL", "BALANCE", "CURRENCY", "BRANCH_CODE")

# read GL_BAL for previous date: keep SQL parameter as string with proper format
filter_date_str = yesterday.strftime("%Y-%m-%d")
sql_query = f"(SELECT CGL,BALANCE,CURRENCY,BRANCH_CODE FROM GL_BALANCE WHERE TRUNC(BALANCE_DATE) = TO_DATE('{filter_date_str}','YYYY-MM-DD')) T1"

df_date_filtered = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", sql_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("fetchsize", 10000) \
    .load()

# union + final aggregation; repartition before final write
combined_df = processed_df.unionByName(df_date_filtered)
final_aggregated_df = combined_df.groupBy("CGL", "CURRENCY", "BRANCH_CODE").agg(F.sum("BALANCE").alias("BALANCE")) \
                                 .orderBy("CGL", "BRANCH_CODE") \
                                 .withColumn("BALANCE_DATE", F.to_date(F.lit(posting_date_str)))

final_aggregated_df = final_aggregated_df.repartition(WRITE_PARTITIONS, "BRANCH_CODE")

final_aggregated_df.write \
    .format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", "GL_BAL") \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .option("batchsize", 10000) \
    .mode("append") \
    .save()

# clean up
spark.catalog.clearCache()


--conf spark.sql.shuffle.partitions=400       # set to about 2-4x total core count
--conf spark.sql.adaptive.enabled=true
--conf spark.sql.adaptive.coalescePartitions.enabled=true
--conf spark.executor.memory=30g
--conf spark.executor.cores=5
--conf spark.executor.instances=10
--conf spark.driver.memory=8g
--conf spark.memory.fraction=0.6
--conf spark.memory.storageFraction=0.3
--conf spark.executor.memoryOverhead=4g
--conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=134217728  # 128MB
